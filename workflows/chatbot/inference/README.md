Chat with the NeuralChat
============

This document showcases the utilization of the fine-tuned model for conversing with NeuralChat. To obtaining the fine-tuned model, please refer to the [fine-tuning](../fine_tuning/README.md) section. The inference of the fine-tuned models has been validated on the 4th Gen Intel® Xeon® Processors, Sapphire Rapids(SPR) and Habana® Gaudi® Deep Learning Processors.

# Prerequisite​

## Environment​

We recommend python 3.9 or higher version. Install the required dependencies using the following command:

```shell
pip install -r requirements.txt
```

# Document Indexing

Document indexing aims to aid users in efficiently parsing locally uploaded files and storing them in a document store for future content retrieval. We have developed two distinct indexing methods: sparse retrieval and dense retrieval. For more information, please consult the [README](./document_indexing/README.md) file.

# Document Reranker

The purpose of a document reranker is to improve the relevance and quality of search results generated by an information retrieval system. It is a component within the inference pipeline that reorders the initially retrieved documents based on their relevance to the user's query. We provided a [ColBERTRanker](./document_ranker/colbert.py) which uses the ColBERT model for reordering search results based on their relevance to a given query. Its purpose is to enhance the accuracy and quality of search results by applying a more sophisticated ranking algorithm.

# Inference

## Inference on Xeon SPR

We provide the [generate.py](./generate.py) script for performing inference on Intel® CPUs. We have enabled IPEX BF16 to speed up the inference. Please use the following commands for inference.

For [MPT](https://huggingface.co/mosaicml/mpt-7b-chat), it uses the gpt-neox-20b tokenizer, so you need to explicitly define it in the command line.

If you don't have a fine-tuned model, please remove the 'peft_model_path' parameter.

```bash
# completion template
python generate.py \
        --base_model_path "mosaicml/mpt-7b-chat" \
        --peft_model_path "./mpt_peft_finetuned_model" \
        --tokenizer_name "EleutherAI/gpt-neox-20b" \
        --use_kv_cache \
        --task completion \
        --instructions "Transform the following sentence into one that shows contrast. The tree is rotten."

# chat template
python generate.py \
        --base_model_path "mosaicml/mpt-7b-chat" \
        --peft_model_path "./mpt_peft_finetuned_model" \
        --tokenizer_name "EleutherAI/gpt-neox-20b" \
        --use_kv_cache \
        --task chat \
        --instructions "Transform the following sentence into one that shows contrast. The tree is rotten."

# summarization template
python generate.py \
        --base_model_path "mosaicml/mpt-7b-chat" \
        --peft_model_path "./mpt_peft_finetuned_model" \
        --tokenizer_name "EleutherAI/gpt-neox-20b" \
        --use_kv_cache \
        --task summarization \
        --instructions "Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the 'forgotten floor,' where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the 'forgotten floor.' Here, inmates with the most severe mental illnesses are incarcerated until they're ready to appear in court. Most often, they face drug charges or charges of assaulting an officer --charges that Judge Steven Leifman says are usually 'avoidable felonies.'"
```

If you want to accelerate the generation, you can use the key/value cache for decoding by adding the flag `--use_kv_cache`, and use jit trace by `pip install optimum-intel` and adding the flag `--jit`.

```bash
python generate.py \
        --base_model_path "mosaicml/mpt-7b-chat" \
        --instructions "Transform the following sentence into one that shows contrast. The tree is rotten." \
        --use_kv_cache \
        --jit
```

The [generate.py](./generate.py) script accepts different arguments to set the inference behavior of the model.

```bash
python generate.py  \
          --temperature 0.2 \
          --top_p 0.8 \
          --top_k 45 \
          --num_beams 1 \
          --repetition_penalty 1.2 \
          --max_new_tokens 512 \
          --base_model_path "mosaicml/mpt-7b-chat" \
          --tokenizer_name "EleutherAI/gpt-neox-20b" \
          --use_kv_cache \
          --instructions "Tell me about Intel Xeon."
```

Here are the explanations of each parameter:
`--temperature`: Controls the diversity of generated text. Lower values result in more deterministic outputs. The default value is 0.1.
`--top_p`: During text generation, only consider tokens with cumulative probability up to this value. This parameter helps to avoid extremely low probability tokens. The default value is 0.75.
`--top_k`: The number of highest probability vocabulary tokens to consider for each step of text generation. The default value is 40.
`--num_beams`: The number of beams to use for beam search decoding. This parameter helps to generate multiple possible completions. The default value is 1.
`--repetition_penalty`: This value controls the penalty for repeating tokens in the generated text. Higher values encourage the model to produce more diverse outputs. The default value is 1.1.
`--max_new_tokens`: The maximum number of tokens allowed in the generated output. This parameter helps to limit the length of the generated text. The default value is 128.


For Llama, use the below command line to chat with it.
If you encounter a failure with the Llama fast tokenizer while using the latest transformers, add the option "--use_slow_tokenizer".
The `tokenizer_class` in `tokenizer_config.json` should be changed from `LLaMATokenizer` to `LlamaTokenizer`.
The `architectures` in `config.json` should be changed from `LLaMAForCausalLM` to `LlamaForCausalLM`.

```bash
python generate.py \
        --base_model_path "decapoda-research/llama-7b-hf" \
        --peft_model_path "./llama_peft_finetuned_model" \
        --use_slow_tokenizer \
        --use_kv_cache \
        --instructions "Transform the following sentence into one that shows contrast. The tree is rotten."
```

```bash
python generate.py \
        --temperature 0.2 \
        --top_p 0.8 \
        --top_k 45 \
        --num_beams 1 \
        --repetition_penalty 1.2 \
        --base_model_path "decapoda-research/llama-7b-hf" \
        --use_slow_tokenizer \
        --use_kv_cache \
        --instructions "Tell me about China."
```
For [Llama2](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) inference, we need use branch [penghuic/chat_mpt_7b](https://github.com/intel/intel-extension-for-transformers/tree/penghuic/chat_mpt_7b/workflows/chatbot/inference) and install IPEX `llm_feature_branch` branch, Please follow these steps to get the quantized model and do inference.
#### Setup
```
WORK_DIR=$PWD
# GCC 12.3 is required, please set it firstly
# Create environment (conda recommended)
conda create -n llm python=3.9 -y
# install deps
conda install gcc=12.3 gxx=12.3 cxx-compiler -c conda-forge -y
conda install cmake ninja mkl mkl-include -y
conda install gperftools -c conda-forge -y

# Install PyTorch
python -m pip install torch==2.1.0.dev20230711+cpu torchvision==0.16.0.dev20230711+cpu torchaudio==2.1.0.dev20230711+cpu --index-url https://download.pytorch.org/whl/nightly/cpu

# Install IPEX with semi-compiler, require gcc 12.3
rm -rf llvm-project && mkdir llvm-project && cd llvm-project
wget https://github.com/llvm/llvm-project/releases/download/llvmorg-16.0.6/cmake-16.0.6.src.tar.xz
wget https://github.com/llvm/llvm-project/releases/download/llvmorg-16.0.6/llvm-16.0.6.src.tar.xz
tar -xf cmake-16.0.6.src.tar.xz && mv cmake-16.0.6.src cmake
tar -xf llvm-16.0.6.src.tar.xz && mv llvm-16.0.6.src llvm
mkdir build && cd build
cmake ../llvm -DCMAKE_INSTALL_PREFIX=${PWD}/_install/llvm -DCMAKE_BUILD_TYPE=Release -DLLVM_TARGETS_TO_BUILD=X86 -DLLVM_INCLUDE_TESTS=OFF -DLLVM_INCLUDE_EXAMPLES=OFF -DLLVM_ENABLE_TERMINFO=OFF -DLLVM_INCLUDE_BENCHMARKS=OFF -DCMAKE_CXX_FLAGS="-D_GLIBCXX_USE_CXX11_ABI=0"
make install -j$(nproc)
ln -s ${PWD}/_install/llvm/bin/llvm-config ${CONDA_PREFIX}/bin/llvm-config-13
cd ../../

git clone --branch llm_feature_branch https://github.com/intel/intel-extension-for-pytorch.git
cd intel_extension_for_pytorch
git submodule sync && git submodule update --init --recursive
export DNNL_GRAPH_BUILD_COMPILER_BACKEND=1
export CXXFLAGS="${CXXFLAGS} -D__STDC_FORMAT_MACROS"
python setup.py install
cd ../

# Install transformers
pip install transformers==4.31.0
# Install others deps
pip install cpuid accelerate datasets sentencepiece protobuf==3.20.3
````
#### Quantization
`meta-llama/Llama-2-7b-chat-hf` model need request the access, please follow the [instruction](https://huggingface.co/meta-llama/Llama-2-7b-hf), the quantized model saved in the `saved_results` folder and named `best_model.pt`.
```
python run_llama_int8.py \
        -m meta-llama/Llama-2-7b-chat-hf \
        --ipex-smooth-quant \
        --dataset "NeelNanda/pile-10k" \
        --output-dir "saved_results" \
        --jit \
        --int8-bf16-mixed
```

#### Inference

```
export KMP_BLOCKTIME=1
export KMP_SETTINGS=1
export KMP_AFFINITY=granularity=fine,compact,1,0
export LD_PRELOAD=${CONDA_PREFIX}/lib/libiomp5.so
export LD_PRELOAD=${LD_PRELOAD}:${CONDA_PREFIX}/lib/libtcmalloc.so

OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <cpu list> python  generate.py \
        --base_model_path meta-llama/Llama-2-7b-chat-hf \
        --use_kv_cache \
        --instructions "Tell me about Intel Xeon." \
        --ipex_int8 \
        --quantized_model_path "./saved_results/best_model.pt"
```

## Deployment on Xeon SPR

Apart from direct inference using the [generate.py](./generate.py) script, you can also run a backend and use the RESTful API for inference. We have enabled IPEX BF16 to speed up the inference.

### Retrieval-free Chatbot

Please refer to the [README](./backend/chat/README.md) for instructions on running the model as a service. Once it is up and running, you will see a generated URL. The RESTful API `worker_generate_stream` can be used to generate text. 

You can use the following command to trigger inference:

```bash
curl -X POST -H "Content-Type: application/json" -d '{"model": "mpt-7b-chat", "prompt": "What are the potential benefits and risks of cryptocurrency investments?"}' http://localhost:80/worker_generate_stream
```

Please make sure to update the URL 'http://localhost:80/worker_generate_stream' with your server's IP address and port.

If you prefer to use the Python API to access the service, you can use the code snippet below:

```python
import requests

url = 'http://localhost:80/worker_generate_stream'
headers = {'Content-Type': 'application/json'}
data = {
    'model': 'mpt-7b-chat',
    'prompt': 'What are the potential benefits and risks of cryptocurrency investments?'
}

response = requests.post(url, headers=headers, json=data)
print(response.json())
```

You can also use [chatcli](../demo/chatcli/) to access the service.

### Retrieval-augmented Chatbot

Please refer to the [README](./backend/fastrag/README.md) for instructions on running the model as a service. Once it is up and running, you will see a generated URL. The RESTful API `fastrag/query` can be used to generate text.

You can use the following command to trigger inference:

``` bash
curl -X POST -H "Content-Type: application/json" -d '{"query": ""Could you elaborate some measures or policies that company is taking on the public rental House and talent housing allowance?", "domain": "ASK_GM", "embedding": "sparse"}' http://localhost:80/fastrag/query
```

You can also use [chatcli](../demo/chatcli/) to access the service.

## Inference on Habana Gaudi

Use this [link](https://docs.habana.ai/en/latest/AWS_EC2_DL1_and_PyTorch_Quick_Start/AWS_EC2_DL1_and_PyTorch_Quick_Start.html) to get started with setting up Gaudi-based Amazon EC2 DL1 instances.

### Setup Habana Environment

```bash
git clone https://github.com/intel/intel-extension-for-transformers.git
cd ./intel-extension-for-transformers/
```

Copy the [generate.py](./generate.py) script to Gaudi instance and place it in the current directory.
Run the Docker container with Habana runtime and necessary environment variables:

```bash
docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host -v $(pwd):/intel-extension-for-transformers vault.habana.ai/gaudi-docker/1.11.0/ubuntu22.04/habanalabs/pytorch-installer-2.0.1:latest
apt-get update
apt-get install git-lfs
git-lfs install
cd /intel-extension-for-transformers/workflows/chatbot/inference/
pip install datasets
pip install optimum
pip install git+https://github.com/huggingface/optimum-habana.git
pip install peft
pip install einops
pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.11.0
```

### Run the inference

You can use the [generate.py](./generate.py) script for performing direct inference on Habana Gaudi instance. We have enabled BF16 to speed up the inference. Please use the following command for inference.

```bash
python generate.py --base_model_path "mosaicml/mpt-7b-chat" \
             --habana \
             --tokenizer_name "EleutherAI/gpt-neox-20b" \
             --use_hpu_graphs \
             --use_kv_cache \
             --instructions "Transform the following sentence into one that shows contrast. The tree is rotten."
```

And you can use `deepspeed` to speedup the inference. currently, TP is not supported for mpt

```bash
python ../utils/gaudi_spawn.py --use_deepspeed --world_size 8 generate.py \
        --base_model_path "mosaicml/mpt-7b-chat" \
        --habana \
        --tokenizer_name "EleutherAI/gpt-neox-20b" \
        --use_hpu_graphs \
        --use_kv_cache \
        --instructions "Transform the following sentence into one that shows contrast. The tree is rotten."
```

Habana supports HPU graph mode for inference speedup, which is available for bloom, gpt2, opt, gptj, gpt_neox, mpt, llama. You can use the parameter `use_hpu_graphs` to speed up the inference.

```bash
python generate.py --base_model_path "EleutherAI/gpt-j-6b" \
             --habana \
             --use_kv_cache \
             --use_hpu_graphs \
             --tokenizer_name "EleutherAI/gpt-j-6b" \
             --instructions "Transform the following sentence into one that shows contrast. The tree is rotten."
```

## Inference with optimized model
Users can follow the [README.md](../../../examples/huggingface/pytorch/text-generation/quantization/README.md) to generate optimized model. Then you can use the following command to do inference.

```bash
export _DNNL_DISABLE_COMPILER_BACKEND=1
# completion template
python generate.py \
        --base_model_path "./Llama-2-7b-chat-hf-int8" \
        --ipex_int8 \
        --use_kv_cache \
        --trust_remote_code \
        --task completion \
        --instructions "Transform the following sentence into one that shows contrast. The tree is rotten."
