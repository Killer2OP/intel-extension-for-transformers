{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeuralChat is a customizable chat framework designed to create user own chatbot within few minutes on multiple architectures. This notebook is used to demostrate how to build a talking chatbot on 3rd Generation of IntelÂ® XeonÂ® Scalable Processors Ice Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/intel/intel-extension-for-transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./intel-extension-for-transformers/\n",
    "!pip install -r requirements.txt\n",
    "%cd ./intel_extension_for_transformers/neural_chat/\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install intel extension for transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../../\n",
    "!pip install v ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch -y\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your chatbot ðŸ’»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving NeuralChat the textual instruction, it will respond with the textual response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 00:35:32.287861: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-30 00:35:32.293245: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-30 00:35:32.293258: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/sdp/miniconda3/envs/neuralchat_notebook/lib/python3.10/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config settings from the environment...\n",
      "Loading model meta-llama/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccf5ea9689f4625aa8df3d5b0632c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "[INST] Tell me about Intel Xeon Scalable Processors. [/INST]  Intel Xeon Scalable Processors are a line of high-performance server processors designed for data center and enterprise computing applications. These processors are part of Intel's \"Xeon\" family, which offers a range of models with different performance and power efficiency characteristics.\n",
      "\n",
      "Here are some key features and benefits of Intel Xeon Scalable Processors:\n",
      "\n",
      "1. Performance: Xeon Scalable Processors are designed to deliver high levels of processing power and performance, making them well-suited for demanding workloads such as virtualization, cloud computing, and big data analytics.\n",
      "2. Power Efficiency: These processors are built with advanced power management technologies, such as Intel's Turbo Boost 3.0 and Dynamic Voltage and Frequency Scaling (DVFS), which help to reduce power consumption and improve energy efficiency.\n",
      "3. Security: Xeon Scalable Processors include a range of security features, such as Intel Software Guard Extensions (SGX) and Intel Trusted Execution Technology (TXT), which help to protect against unauthorized access and malicious attacks.\n"
     ]
    }
   ],
   "source": [
    "from intel_extension_for_transformers.neural_chat import build_chatbot\n",
    "chatbot = build_chatbot()\n",
    "response = chatbot.predict(\"Tell me about Intel Xeon Scalable Processors.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLI command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sdp/miniconda3/envs/neuralchat_notebook/lib/python3.10/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "Loading config settings from the environment...\n",
      "Loading model meta-llama/Llama-2-7b-chat-hf\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.54it/s]\n",
      "Model loaded.\n",
      "[INST] Tell me about Intel Xeon Scalable Processors. [/INST]  Intel Xeon Scalable Processors are a line of high-performance server processors designed for data center and enterprise computing applications. These processors are part of Intel's \"Xeon\" family, which offers a range of models with different performance and power efficiency characteristics.\n",
      "\n",
      "Here are some key features and benefits of Intel Xeon Scalable Processors:\n",
      "\n",
      "1. Performance: Xeon Scalable Processors are designed to deliver high levels of processing power and performance, making them well-suited for demanding workloads such as virtualization, cloud computing, and big data analytics.\n",
      "2. Power Efficiency: These processors are built with advanced power management technologies, such as Intel's Turbo Boost 3.0 and Dynamic Voltage and Frequency Scaling (DVFS), which help to reduce power consumption and improve energy efficiency.\n",
      "3. Memory Support: Xeon Scalable Processors support up to 560 lanes of PCIe 4.0 bandwidth, allowing for faster memory access and improved scalability.\n",
      "4.\n"
     ]
    }
   ],
   "source": [
    "!neuralchat predict --query \"Tell me about Intel Xeon Scalable Processors.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Chat With Retrieval Plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User could also leverage NeuralChat Retrieval plugin to do domain specific chat by feding with some documents like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 00:37:41,725 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create retrieval plugin instance...\n",
      "plugin parameters:  {'input_path': '../../assets/docs/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 00:37:42,877 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n",
      "2023-10-30 00:37:43,098 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8132f2ca0240422ca23c36d7fa6a55e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The local knowledge base has been successfully built!\n",
      "Loading model Intel/neural-chat-7b-v1-1\n",
      "Model loaded.\n",
      "Chat with AI Agent.\n"
     ]
    }
   ],
   "source": [
    "from intel_extension_for_transformers.neural_chat import PipelineConfig\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot\n",
    "from intel_extension_for_transformers.neural_chat import plugins\n",
    "plugins.retrieval.enable=True\n",
    "plugins.retrieval.args[\"input_path\"]=\"../../assets/docs/\"\n",
    "config = PipelineConfig(plugins=plugins, model_name_or_path='Intel/neural-chat-7b-v1-1')\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(\"How many cores does the IntelÂ® XeonÂ® Platinum 8480+ Processor have in total?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voice Chat with ASR & TTS Plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of voice chat, users have the option to engage in various modes: utilizing input audio and receiving output audio, employing input audio and receiving textual output, or providing input in textual form and receiving audio output.\n",
    "\n",
    "For the Python API code, users have the option to enable different voice chat modes by setting ASR and TTS plugins enable or disable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2816  100  2816    0     0  16769      0 --:--:-- --:--:-- --:--:-- 16862\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 48172  100 48172    0     0   249k      0 --:--:-- --:--:-- --:--:--  250k\n"
     ]
    }
   ],
   "source": [
    "!curl -OL https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/assets/speaker_embeddings/spk_embed_default.pt\n",
    "!curl -OL https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/assets/audio/sample.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create tts plugin instance...\n",
      "plugin parameters:  {'output_audio_path': './response.wav'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 00:39:39,560 - speechbrain.pretrained.fetching - INFO - Fetch hyperparams.yaml: Using existing file/symlink in /tmp/speechbrain/spkrec-xvect-voxceleb/hyperparams.yaml.\n",
      "2023-10-30 00:39:39,560 - speechbrain.pretrained.fetching - INFO - Fetch custom.py: Delegating to Huggingface hub, source speechbrain/spkrec-xvect-voxceleb.\n",
      "2023-10-30 00:39:39,710 - speechbrain.pretrained.fetching - INFO - Fetch embedding_model.ckpt: Using existing file/symlink in /tmp/speechbrain/spkrec-xvect-voxceleb/embedding_model.ckpt.\n",
      "2023-10-30 00:39:39,711 - speechbrain.pretrained.fetching - INFO - Fetch mean_var_norm_emb.ckpt: Using existing file/symlink in /tmp/speechbrain/spkrec-xvect-voxceleb/mean_var_norm_emb.ckpt.\n",
      "2023-10-30 00:39:39,712 - speechbrain.pretrained.fetching - INFO - Fetch classifier.ckpt: Using existing file/symlink in /tmp/speechbrain/spkrec-xvect-voxceleb/classifier.ckpt.\n",
      "2023-10-30 00:39:39,712 - speechbrain.pretrained.fetching - INFO - Fetch label_encoder.txt: Using existing file/symlink in /tmp/speechbrain/spkrec-xvect-voxceleb/label_encoder.ckpt.\n",
      "2023-10-30 00:39:39,712 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create asr plugin instance...\n",
      "plugin parameters:  {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 00:39:42,239 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create retrieval plugin instance...\n",
      "plugin parameters:  {'input_path': '../../assets/docs/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 00:39:43,023 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4229f3161ab4a2cb3a8b3677dd40a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The local knowledge base has been successfully built!\n",
      "Loading model Intel/neural-chat-7b-v1-1\n",
      "Model loaded.\n",
      "generated text in 0.5537045001983643 seconds, and the result is: who is pat gelsinger\n",
      "Chat with AI Agent.\n",
      "assistant\n",
      "I don't know this person.#---#\n",
      "## --- ### --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- #\n",
      "[\"assistant I don't know this person.# # ## ### ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## ## #.\"]\n",
      "No customized speaker embedding or default embedding are found! Use the backup one\n",
      "./response.wav\n"
     ]
    }
   ],
   "source": [
    "from intel_extension_for_transformers.neural_chat import PipelineConfig\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot\n",
    "from intel_extension_for_transformers.neural_chat import plugins\n",
    "plugins.tts.enable = True\n",
    "plugins.tts.args[\"output_audio_path\"] = \"./response.wav\"\n",
    "plugins.asr.enable = True\n",
    "\n",
    "config = PipelineConfig(plugins=plugins, model_name_or_path='Intel/neural-chat-7b-v1-1')\n",
    "chatbot = build_chatbot(config)\n",
    "result = chatbot.predict(query=\"./sample.wav\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can display the generated wav file using IPython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create tts plugin instance...\n",
      "plugin parameters:  {'output_audio_path': './response.wav'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 00:40:59,520 - speechbrain.pretrained.fetching - INFO - Fetch hyperparams.yaml: Using existing file/symlink in /tmp/speechbrain/spkrec-xvect-voxceleb/hyperparams.yaml.\n",
      "2023-10-30 00:40:59,521 - speechbrain.pretrained.fetching - INFO - Fetch custom.py: Delegating to Huggingface hub, source speechbrain/spkrec-xvect-voxceleb.\n",
      "2023-10-30 00:40:59,675 - speechbrain.pretrained.fetching - INFO - Fetch embedding_model.ckpt: Using existing file/symlink in /tmp/speechbrain/spkrec-xvect-voxceleb/embedding_model.ckpt.\n",
      "2023-10-30 00:40:59,676 - speechbrain.pretrained.fetching - INFO - Fetch mean_var_norm_emb.ckpt: Using existing file/symlink in /tmp/speechbrain/spkrec-xvect-voxceleb/mean_var_norm_emb.ckpt.\n",
      "2023-10-30 00:40:59,676 - speechbrain.pretrained.fetching - INFO - Fetch classifier.ckpt: Using existing file/symlink in /tmp/speechbrain/spkrec-xvect-voxceleb/classifier.ckpt.\n",
      "2023-10-30 00:40:59,676 - speechbrain.pretrained.fetching - INFO - Fetch label_encoder.txt: Using existing file/symlink in /tmp/speechbrain/spkrec-xvect-voxceleb/label_encoder.ckpt.\n",
      "2023-10-30 00:40:59,677 - speechbrain.utils.parameter_transfer - INFO - Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create asr plugin instance...\n",
      "plugin parameters:  {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 00:41:02,335 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create retrieval plugin instance...\n",
      "plugin parameters:  {'input_path': '../../assets/docs/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 00:41:03,115 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ebc7bca1ebb474da92a8794dd28c0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The local knowledge base has been successfully built!\n",
      "Loading model Intel/neural-chat-7b-v1-1\n",
      "Model loaded.\n",
      "generated text in 0.32239818572998047 seconds, and the result is: who is pat gelsinger\n",
      "Chat with AI Agent.\n",
      "assistant\n",
      "I don't know this person.#---#\n",
      "## --- ### Gelsinger #\n",
      "\n",
      "Patrick Joseph \"Pat\" Gelsinger (born August 29, 1956) is an American business executive and former politician who served as the United States Secretary of Defense from 2021 until 2023 under President Joe Biden.[10] He previously served in the same role during the Obama administration,[11][12] making him the second person after Robert Gates to have held both positions simultaneously since 1947;[13] he was also the first secretary of defense to have been born in the 20th century.[14] Prior to his appointment at age 65, Gelsinger had never held elected office or worked for the federal government before being appointed by thenâ€“Senator Chuck Hagel.[15] As president, Barack Obama nominated Gelsinger on December 23, 2014,[16] and the U.S. Senate confirmed him on January 26, 2015.[17]###############\n",
      "['assistant I don\\'t know this person.# # ## ### Gelsinger # Patrick Joseph \"Pat\" Gelsinger (born August twenty-nine, 1956) is an American business executive and former politician who served as the United States Secretary of Defense from two thousand and twenty-one until two thousand and twenty-three under President Joe Biden.', '[10] He previously served in the same role during the Obama administration,[11][12] making him the second person after Robert Gates to have held both positions simultaneously since 1947;[13] he was also the first secretary of defense to have been born in the twentieth century.', '[14] Prior to his appointment at age sixty-five, Gelsinger had never held elected office or worked for the federal government before being appointed by thenâ€“Senator Chuck Hagel.[15] As president, Barack Obama nominated Gelsinger on December twenty-three, 2014,[16] and the u . ess . Senate confirmed him on January twenty-six, 2015.[17]###############.']\n",
      "No customized speaker embedding or default embedding are found! Use the backup one\n",
      "./response.wav\n"
     ]
    }
   ],
   "source": [
    "from intel_extension_for_transformers.neural_chat import PipelineConfig\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot\n",
    "from intel_extension_for_transformers.neural_chat import plugins\n",
    "plugins.tts.enable = True\n",
    "plugins.tts.args[\"output_audio_path\"] = \"./response.wav\"\n",
    "plugins.asr.enable = True\n",
    "\n",
    "config = PipelineConfig(plugins=plugins, model_name_or_path='Intel/neural-chat-7b-v1-1')\n",
    "chatbot = build_chatbot(config)\n",
    "result = chatbot.predict(query=\"./sample.wav\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
